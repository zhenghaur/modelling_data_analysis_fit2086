install.packages('caret')
library(caret)
# Question 3
library(kknn)
library(boot)
source("my.prediction.stats.R")
# Question 3.1
# read files
ms.truth <- read.csv("ms.truth.2022.csv", stringsAsFactors = T)
ms.measured <- read.csv("ms.measured.2022.csv", stringsAsFactors = T)
# k = 25
k <- c(1:25)
# computing rmse
rmse <- k
for (i in rmse){
ytest.hat <- fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = i, kernel = 'optimal'))
rmse[i] <- sqrt(mean((ytest.hat - ms.truth$intensity)^2))
}
# plotting the graph
plot(k, rmse, type = 'l', xlab = "k", ylab = "RMSE", main = "RMSE against k")
# Question 3.2
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 2)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 2, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 2)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 6)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 6, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 6)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 12)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 12, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 12)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 25)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 25, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 25)"), fill=c(1, 2, 4))
# Question 3.5
# estimate best value of k
train.kknn(intensity ~ .,data = ms.measured, kmax = 25,kernel = "optimal")
# RMSE of k = 5
rmse[5]
# RMSE of k = 6
rmse[6]
# Question 3.6
# difference of truth and estimate
diff <- ms.truth$intensity - fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 5, kernel = 'optimal'))
# standard deviation of difference
sd(diff)
# Question 3.7
# index of maximum estimated intensity
ind <- which.max(fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 5, kernel = 'optimal')))
# MZ of maximum estimated intensity
ms.measured[ind,]
# setwd
setwd("C:/Users/lzhau/Documents/Monash Uni/2022S2/FIT2086/assignment 3")
# Question 3
library(kknn)
library(boot)
source("my.prediction.stats.R")
# Question 3.1
# read files
ms.truth <- read.csv("ms.truth.2022.csv", stringsAsFactors = T)
ms.measured <- read.csv("ms.measured.2022.csv", stringsAsFactors = T)
# k = 25
k <- c(1:25)
# computing rmse
rmse <- k
for (i in rmse){
ytest.hat <- fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = i, kernel = 'optimal'))
rmse[i] <- sqrt(mean((ytest.hat - ms.truth$intensity)^2))
}
# plotting the graph
plot(k, rmse, type = 'l', xlab = "k", ylab = "RMSE", main = "RMSE against k")
# Question 3.2
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 2)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 2, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 2)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 6)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 6, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 6)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 12)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 12, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 12)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 25)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 25, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 25)"), fill=c(1, 2, 4))
# Question 3.5
# estimate best value of k
train.kknn(intensity ~ .,data = ms.measured, kmax = 25,kernel = "optimal")
# RMSE of k = 5
rmse[5]
# RMSE of k = 6
rmse[6]
# Question 3.6
# difference of truth and estimate
diff <- ms.truth$intensity - fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 5, kernel = 'optimal'))
# standard deviation of difference
sd(diff)
# Question 3.7
# index of maximum estimated intensity
ind <- which.max(fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 5, kernel = 'optimal')))
# MZ of maximum estimated intensity
ms.measured[ind,]
# Question 3.8
# k = 5
boot.5 <- function(formula, data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# compute the prediction for MZ = 8818 and return it
return (fitted(kknn(intensity ~ ., d, ms.truth[ind,], k = 5, kernel = "optimal")))
}
bs.5 <- boot(data = ms.measured, statistic = boot.5, R = 5000)
# Question 3.8
# k = 5
boot.5 <- function(formula, data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# compute the prediction for MZ = 8818 and return it
return (fitted(kknn(intensity ~ ., d, ms.truth[ind,], k = 5, kernel = "optimal")))
}
bs.5 <- boot(data = ms.measured, statistic = boot.5, R = 5000)
# k = 5
boot.5 <- function(data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# compute the prediction for MZ = 8818 and return it
return (fitted(kknn(intensity ~ ., d, ms.truth[ind,], k = 5, kernel = "optimal")))
}
bs.5 <- boot(data = ms.measured, statistic = boot.5, R = 5000)
# 95% confidence interval
boot.ci(bs.5, conf = 0.95, type = "bca")
# Question 2
source("my.prediction.stats.R")
source("wrappers.R")
library(glmnet)
library(rpart)
library(boot)
# Question 2.1
# read file
heart.train <- read.csv("heart.train.ass3.2022.csv", stringsAsFactor = T)
# fitting a decision tree
tree.heart <- rpart(HD ~ ., heart.train)
# cross-validation with 10 folds and 5000 repetitions
cv <- learn.tree.cv(HD ~ .,data = heart.train, nfolds = 10, m = 5000)
# best tree
cv$best.tree
# Question 2.2
# plotting the tree
plot(cv$best.tree)
text(cv$best.tree, pretty = 12)
# Question 2.3
cv$best.tree
# Question 2.2
# plotting the tree
plot(cv$best.tree)
text(cv$best.tree, pretty = 12)
# Question 2.3
cv$best.tree
rm(list = ls())
# setwd
setwd("C:/Users/lzhau/Documents/Monash Uni/2022S2/FIT2086/assignment 3")
# Question 1
# Question 1.1
# read file
df = read.csv("fuel.ass3.2022.csv", stringsAsFactors = T)
# fitting a linear model to the data
fit <- lm(Comb.FE ~ ., data = df)
# view summary of data
summary(fit)
# Question 1.2
# critical value
a = 0.05
# number of comparisons
n = 17
# adjusted critical value
a = a/n
a
# Question 1.3
# fitting linear model to only engine displacement
fit.Eng.Displacement <- lm(Comb.FE ~ Eng.Displacement, data = df)
fit.Eng.Displacement
# fitting liear model to only drive systems
fit.Drive.Sys <- lm(Comb.FE ~ Drive.Sys, data = df)
fit.Drive.Sys
# mean efficiency for Front-wheel drive
9.1245 + 4.0422
# Question 1.4
# stepwise selection procedure
fit.bic <- step(fit, k = log(500), direction = "both")
fit.bic
# Question 1.5
# predicting for row 33
predict(fit.bic, newdata = df[33,], interval = "confidence")
# Question 2
source("my.prediction.stats.R")
source("wrappers.R")
library(glmnet)
library(rpart)
library(boot)
# Question 2.1
# read file
heart.train <- read.csv("heart.train.ass3.2022.csv", stringsAsFactor = T)
# fitting a decision tree
tree.heart <- rpart(HD ~ ., heart.train)
# cross-validation with 10 folds and 5000 repetitions
cv <- learn.tree.cv(HD ~ .,data = heart.train, nfolds = 10, m = 5000)
# best tree
cv$best.tree
# Question 2.2
# plotting the tree
plot(cv$best.tree)
text(cv$best.tree, pretty = 12)
# Question 2.3
cv$best.tree
# Question 2.5
# fitting a logistic regression model
fullmod <- glm(HD ~ ., data = heart.train, family = binomial)
# stepwise selection procedure
fullmod.kic <- step(fullmod, k = 3, direction = "both")
summary(fullmod.kic)
# Question 2.8
# read file
heart.test <- read.csv("heart.test.ass3.2022.csv", stringsAsFactor = T)
# compute prediction statistics
my.pred.stats(predict(cv$best.tree, heart.test)[,2], heart.test$HD)
my.pred.stats(predict(fullmod.kic, heart.test, type = "response"), heart.test$HD)
# Question 2.9a
# predicting using tree
predict(cv$best.tree, heart.test[10,])
# Question 2.9b
# predicting using step-wise logistic regression
predict(fullmod.kic, heart.test[10,], type = "response")
# Question 2.10
boot.65th <- function(formula, data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# Fit a logistic regression to the bootstrapped data
fit <- glm(formula, d, family = binomial)
# Compute the prediction for 65th patient and return it
return (predict(fit, heart.test[65,], type = "response"))
}
bs.65th <- boot(data = heart.train, statistic = boot.65th, R = 5000, formula = fullmod.kic)
boot.ci(bs.65th, conf = 0.95, type = "bca")
boot.66th <- function(formula, data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# Fit a logistic regression to the bootstrapped data
fit <- glm(formula, d, family = binomial)
# Compute the prediction for 66th patient and return it
return (predict(fit, heart.test[66,], type = "response"))
}
bs.66th <- boot(data = heart.train, statistic = boot.66th, R = 5000, formula = fullmod.kic)
boot.ci(bs.66th, conf = 0.95, type = "bca")
# Question 3
library(kknn)
library(boot)
source("my.prediction.stats.R")
# Question 3.1
# read files
ms.truth <- read.csv("ms.truth.2022.csv", stringsAsFactors = T)
ms.measured <- read.csv("ms.measured.2022.csv", stringsAsFactors = T)
# k = 25
k <- c(1:25)
# computing rmse
rmse <- k
for (i in rmse){
ytest.hat <- fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = i, kernel = 'optimal'))
rmse[i] <- sqrt(mean((ytest.hat - ms.truth$intensity)^2))
}
# plotting the graph
plot(k, rmse, type = 'l', xlab = "k", ylab = "RMSE", main = "RMSE against k")
# Question 3.2
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 2)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 2, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 2)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 6)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 6, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 6)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 12)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 12, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 12)"), fill=c(1, 2, 4))
# training data points
plot(ms.measured, pch = '*', col = 1, xlab = "MZ", ylab = "intensity")
# true spectrum
lines(ms.truth, type = 'l', lwd = 3, col = 2)
# estimated spectrum (k = 25)
lines(ms.measured$MZ, fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 25, kernel = 'optimal')),
type = 'l', lwd = 2, col = 4)
# legends
legend(x="topright",legend = c("training data points","true spectrum","estimated spectrum (k = 25)"), fill=c(1, 2, 4))
# Question 3.5
# estimate best value of k
train.kknn(intensity ~ .,data = ms.measured, kmax = 25,kernel = "optimal")
# RMSE of k = 5
rmse[5]
# RMSE of k = 6
rmse[6]
# Question 3.6
# difference of truth and estimate
diff <- ms.truth$intensity - fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 5, kernel = 'optimal'))
# standard deviation of difference
sd(diff)
# Question 3.7
# index of maximum estimated intensity
ind <- which.max(fitted(kknn(intensity ~ ., ms.measured, ms.truth, k = 5, kernel = 'optimal')))
# MZ of maximum estimated intensity
ms.measured[ind,]
# Question 3.8
# k = 5
boot.5 <- function(formula, data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# compute the prediction for MZ = 8818 and return it
return (fitted(kknn(formula, d, ms.truth[ind,], k = 5, kernel = "optimal")))
}
bs.5 <- boot(data = ms.measured, statistic = boot.5, R = 5000, formula = intensity ~ .)
# k = 3
boot.3 <- function(formula, data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# compute the prediction for MZ = 8818 and return it
return (fitted(kknn(formula, d, ms.truth[ind,], k = 3, kernel = "optimal")))
}
bs.3 <- boot(data = ms.measured, statistic = boot.3, R = 5000, formula = intensity ~ .)
# k = 20
boot.20 <- function(formula, data, indices)
{
# Create a bootstrapped version of our data
d <- data[indices,]
# compute the prediction for MZ = 8818 and return it
return (fitted(kknn(formula, d, ms.truth[ind,], k = 20, kernel = "optimal")))
}
bs.20 <- boot(data = ms.measured, statistic = boot.20, R = 5000, formula = intensity ~ .)
# 95% confidence interval
boot.ci(bs.5, conf = 0.95, type = "bca")
boot.ci(bs.3, conf = 0.95, type = "bca")
boot.ci(bs.20, conf = 0.95, type = "bca")
